
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
/* Stolen from Sergey and Jon Barron */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strongred {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 14px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }


  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Abhishek Kar</title>
  <meta name="Abhishek Kar's Homepage"http-equiv="Content-Type" content="Abhishek Kar's Homepage">

  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src='https://www.google.com/recaptcha/api.js'></script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]//function(){
    (i[r].q=i[r].q//[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-41102261-2', 'auto');
    ga('send', 'pageview');
</script>
</head>

<body>
  <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center"><font size="7">Abhishek Kar</font><br>
          <tr>
            <td width="67%" valign="middle" align="justify">



		    <p>I am currently a Senior Research Scientist at <a href="https://arvr.google.com/">Google</a> where I work on problems at the intersection of 3D computer vision and machine learning.<br>

Prior to this, I was the Machine Learning Lead at <a href="https://fyusion.com">Fyusion Inc.</a>, a 3D computational photography startup based in San Francisco. I graduated from UC Berkeley in 2017 from <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik's</a> group working on Machine Learning and 3D Computer Vision. Before moving to Berkeley, I completed my undergrad at <a href="http://www.iitk.ac.in">IIT Kanpur</a> where I worked with <a href="http://www.cse.iitk.ac.in/users/amit/">Prof. Amitabha Mukerjee</a> and <a href="http://research.microsoft.com/en-us/um/people/sumitg/">Dr. Sumit Gulwani</a> on computer vision and intelligent tutoring systems. I have also spent time at <a href="http://research.microsoft.com/en-us/">Microsoft Research</a> working on viewing large imagery on mobile devices and with the awesome team at <a href="http://fyusion.com">Fyusion</a> capturing "3D photos" with mobile devices and developing deep learning models for them. Some features I have shipped/worked on at Fyusion include 3D visual search, creation of <a href="https://www.youtube.com/watch?v=j8GAQYh32iU">user generated AR/VR content</a> and <a href="https://www.youtube.com/watch?v=SRNaq6h0X4gff">real-time style transfer on mobile devices</a>.
<br>

<p align=center>
<a href="mailto:akar@berkeley.edu">Email</a> / <a href="#">CV</a> / <a href="http://scholar.google.com/citations?user=TIpmrtoAAAAJ&hl=en">Google Scholar</a> / <a href="http://www.linkedin.com/in/abhishekkar/"> LinkedIn </a> / <a href="https://github.com/akar43">Github</a> / <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-199.html">Thesis</a>
</p>
            </td>
            <td width="33%"valign="top"><img src="images/AK_headshot_circular.png" width="100%"></td>
          </tr>
        </table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
            <h2>News</h2>
            <ul>
		    <li>Moved to <a href="https://arvr.google.com/">Google</a> as a senior research scientist in Jan 2020. </li>
	    <li> Our <a href="https://fyusion.com/immersive3d">SIGGRAPH work</a> on photorealistic free viewpoint rendering was featured on <a href="https://www.forbes.com/sites/charliefink/2019/07/26/thisweek-in-xr-are-growth-predictions-coming-true">Forbes</a>, <a href="https://venturebeat.com/2019/07/25/fyusion-unveils-3d-imaging-tech-for-marketers-to-show-off-photorealistic-product-views/">VentureBeat</a> and more!</li>
	    <li> <a href="https://fyusion.com/LLFF">Local Light Field Fusion</a> accepted at SIGGRAPH 2019.</li>
	    <li> <a href="https://drive.google.com/open?id=1mTBOJefYp6_ZkWwgshcikQ_AI5nw1t7_">Paper</a> on 3D sceneflow prediction accepted at CVPR 2019.</li>	    
            <li> <b>Dec 2017:</b> <a href="images/graduation.jpg">Graduated</a> from Berkeley. Started full-time at Fyusion.</li>
            </ul>
		</td></tr>
        <tr><td>
            <h2>Publications</h2>
            <p align="justify">I'm interested in 3D computer vision - more specifically inferring 3D shape (and related 3D properties) from image collections in the wild. More recently, I have also been interested in computational photography and producing immersive visual content using 3D inferred from images.</p>
		</td></tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="33%" valign="top"><a href="llff.gif"><img src="llff.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://fyusion.com/LLFF" id="">
              <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none">
              <heading>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</heading></a><br>
              <a href="http://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall*</a>, <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan*</a>, <a href="https://scholar.google.com/citations?user=yZMAlU4AAAAJ">Rodrigo Ortiz-Cayon</a>, 
              <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>, <strong>Abhishek Kar</strong><br>
              <em>SIGGRAPH</em>, 2019
              <br></p>

              <div class="paper" id="LLFF">
                <a href="https://arxiv.org/abs/1905.00889">paper</a> /
                <a href="https://fyusion.com/LLFF">project</a> /
                <a href="https://github.com/Fyusion/LLFF">code</a> /
                <a href="https://youtu.be/LY6MgDUzS3M">video</a> /
                <a href="javascript:toggleblock('llff_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('LLFF')" class="togglebib">bibtex</a> /
		<a href="https://venturebeat.com/2019/07/25/fyusion-unveils-3d-imaging-tech-for-marketers-to-show-off-photorealistic-product-views/">p</a> <a href="https://www.forbes.com/sites/charliefink/2019/07/26/thisweek-in-xr-are-growth-predictions-coming-true">r</a> <a href="https://vfxscience.com/2019/07/27/fyusion-light-field-tech/">e</a> <a href="https://mixed.de/siggraph-2019-fyusion-zeigt-ki-gestuetztes-smartohne-3d-scanning">s</a> <a href="https://www.spar3d.com/uncategorized/fyusion-uses-machine-learning-to-render-realistic-3d-product-views/">s</a>
		      

                <p align="justify"> <i id="llff_abs">We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.</i></p>
  
                <pre xml:space="preserve">
@article{mildenhall2019llff,
  title={Local Light Field Fusion: Practical View 
  Synthesis with Prescriptive Sampling Guidelines},
  author={Ben Mildenhall and 
  Pratul P. Srinivasan and 
  Rodrigo Ortiz-Cayon and 
  Nima Khademi Kalantari and 
  Ravi Ramamoorthi and 
  Ren Ng and 
  Abhishek Kar},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019}
}

                  </pre>
                </div>
            </td>

        </tr>
          
          <tr>
            <td width="33%" valign="top"><a href="sceneflow/sf-teaser.jpg"><img src="sceneflow/sf-teaser.jpg" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://drive.google.com/open?id=1mTBOJefYp6_ZkWwgshcikQ_AI5nw1t7_" id="SF">
              <!-- <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Learning Independent Object Motion from Unlabelled Stereoscopic Videos</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~zhecao/">Zhe Cao</a>, <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~chaene/">Christian H&auml;ne</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019
              <br></p>

              <div class="paper" id="sf">
		            <a href="https://people.eecs.berkeley.edu/~zhecao/sceneflow/">project</a> /
                <a href="javascript:toggleblock('sf_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('sf')" class="togglebib">bibtex</a> /
                <a href="https://arxiv.org/abs/1901.01971">arxiv</a>

                <p align="justify"> <i id="sf_abs">We present a system for learning motion maps of independently moving objects from stereo videos. The only annotations used in our system are 2D object bounding boxes which introduce the notion of objects in our system. Unlike prior learning based approaches which have focused on predicting dense optical flow fields and/or depth maps for images, we propose to predict instance specific 3D scene flow maps and instance masks from which we derive a factored 3D motion map for each object instance. Our network takes the 3D geometry of the problem into account which allows it to correlate the input images and distinguish moving objects from static ones. We present experiments evaluating the accuracy of our 3D flow vectors, as well as depth maps and projected 2D optical flow where our jointly learned system outperforms earlier approaches trained for each task independently.</i></p>
  
                <pre xml:space="preserve">
@incollection{sfCaoKHM2019,
author = {Zhe Cao and
Abhishek Kar and
Christian H\"ane and
Jitendra Malik},
title = {Learning Independent Object Motion 
from Unlabelled Stereoscopic Videos},
booktitle = CVPR,
year = {2019},
}
  
                  </pre>
                </div>
            </td>

        </tr>
          
          <tr>
            <td width="33%" valign="top"><a href="lsm-teaser.png"><img src="lsm-teaser.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="lsm/lsm_nips17.pdf" id="LSM">
              <!-- <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Learning a Multi-View Stereo Machine</heading></a><br>
              <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~chaene/">Christian H&auml;ne</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Neural Information Processing Systems(NIPS)</em>, 2017
              <br></p>

              <div class="paper" id="lsm">
                <a href="javascript:toggleblock('lsm_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('lsm')" class="togglebib">bibtex</a> /
                <a href="lsm/supplsm.pdf">supplementary</a> /
                <a href="https://arxiv.org/abs/1708.05375">arxiv</a> /
                <a href="http://bair.berkeley.edu/blog/2017/09/05/unified-3d/">blog</a> /
                <a href="https://github.com/akar43/lsm">code</a>

  
                <p align="justify"> <i id="lsm_abs">We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.</i></p>
  
                <pre xml:space="preserve">
@incollection{lsmKarHM2017,
  author = {Abhishek Kar and
  Christian H\"ane and
  Jitendra Malik},
  title = {Learning a Multi-View Stereo Machine},
  booktitle = NIPS,
  year = {2017},
  }
  
                  </pre>
                </div>
            </td>

        </tr>

		    <tr>
            <td width="33%" valign="top"><a href="meanshapes.png"><img src="meanshapes.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="pamishapes.pdf" id="PamiShapes">
              <heading>Learning Category-Specific Deformable 3D Models for Object Reconstruction</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani*</a>, <strong>Abhishek Kar*</strong>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2017
              <br></p>

              <div class="paper" id="pamishapes">
              <a href="javascript:toggleblock('pamishapes_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('pamishapes')" class="togglebib">bibtex</a> /
              <a href="categoryShapes/" class="togglebib">project</a>

              <p align="justify"> <i id="pamishapes_abs">We address the problem of fully automatic object localization and reconstruction from a single image. This is both a very challenging and very important problem which has, until recently, received limited attention due to difficulties in segmenting objects and predicting their poses. Here we leverage recent advances in learning convolutional networks for object detection and segmentation and introduce a complementary network for the task of camera viewpoint prediction. These predictors are very powerful, but still not perfect given the stringent requirements of shape reconstruction. Our main contribution is a new class of deformable 3D models that can be robustly fitted to images based on noisy pose and silhouette estimates computed upstream and that can be learned directly from 2D annotations available in object detection datasets. Our models capture top-down information about the main global modes of shape variation within a class providing a ``low-frequency'' shape. In order to capture fine instance-specific shape details, we fuse it with a high-frequency component recovered from shading cues. A comprehensive quantitative analysis and ablation study on  the PASCAL 3D+ dataset validates the approach as we show fully automatic reconstructions on PASCAL VOC as well as large improvements on the task of viewpoint prediction.</i></p>

              <pre xml:space="preserve">
@article{pamishapeTulsianiKCM15,
author = {Shubham Tulsiani and
Abhishek Kar and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Learning Category-Specific Deformable 3D
Models for Object Reconstruction},
journal = {TPAMI},
year = {2016},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="3r.png"><img src="3r.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <heading>The three R's of computer vision: Recognition, reconstruction and reorganization</heading><br>
              Jitendra Malik, Pablo Arbelaez, Jo&atilde;o Carreira, Katerina Fragkiadaki, <br>Ross Girshick, Georgia Gkioxari, Saurabh Gupta, Bharath Hariharan, <strong>Abhishek Kar</strong>, Shubham Tulsiani<br>
              <em>Pattern Recognition Letters</em>, 2016
              <br></p>

              <div class="paper" id="3rcv">
              <a href="http://www.sciencedirect.com/science/article/pii/S0167865516000313">paper</a> /
              <a href="javascript:toggleblock('3rcv_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('3rcv')" class="togglebib">bibtex</a>

              <p align="justify"> <i id="3rcv_abs">We argue for the importance of the interaction between recognition, reconstruction and re-organization, and propose that as a unifying framework for computer vision. In this view, recognition of objects is reciprocally linked to re-organization, with bottom-up grouping processes generating candidates, which can be classified using top down knowledge, following which the segmentations can be refined again. Recognition of 3D objects could benefit from a reconstruction of 3D structure, and 3D reconstruction can benefit from object category-specific priors. We also show that reconstruction of 3D structure from video data goes hand in hand with the reorganization of the scene. We demonstrate pipelined versions of two systems, one for RGB-D images, and another for RGB images, which produce rich 3D scene interpretations in this framework.</i></p>

              <pre xml:space="preserve">
@article{malik2016three,
title={The three R's of computer vision:
  Recognition, reconstruction and reorganization},
author={Malik, Jitendra and
  Arbel{\'a}ez, Pablo and
  Carreira, Jo{\~a}o and
Fragkiadaki, Katerina and
Girshick, Ross and
Gkioxari, Georgia and
Gupta, Saurabh and
Hariharan, Bharath and
Kar, Abhishek and
Tulsiani, Shubham},
journal={Pattern Recognition Letters},
volume={72},
pages={4--14},
year={2016},
publisher={North-Holland}
}

                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="symmetry.png"><img src="symmetry.png" alt="sym" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="shapesymmetry.pdf" id="ShapeSym">
              <heading>Shape and Symmetry Induction for 3D Objects</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <strong>Abhishek Kar</strong>, <a href="http://ttic.uchicago.edu/~huangqx">Qixing Huang</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>arXiv:1511.07845</em>, 2015
              <br></p>

              <div class="paper" id="shapesym">
              <a href="javascript:toggleblock('shapesym_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('shapesym')" class="togglebib">bibtex</a> /
              <a href="https://arxiv.org/abs/1511.07845">arxiv</a>

              <p align="justify"> <i id="shapesym_abs">Actions as simple as grasping an object or navigating around it require a rich understanding of that object's 3D shape from a given viewpoint. In this paper we repurpose powerful learning machinery, originally developed for object classification, to discover image cues relevant for recovering the 3D shape of potentially unfamiliar objects. We cast the problem as one of local prediction of surface normals and global detection of 3D reflection symmetry planes, which open the door for extrapolating occluded surfaces from visible ones. We demonstrate that our method is able to recover accurate 3D shape information for classes of objects it was not trained on, in both synthetic and real images.</i></p>

              <pre xml:space="preserve">
@incollection{shapeSymTulsianiKHCM15,
author = {Shubham Tulsiani and
Abhishek Kar and
Qixing Huang and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Shape and Symmetry Induction
for 3D Objects},
booktitle = arxiv:1511.07845,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="amodal_highres.png"><img src="amodal.png" alt="amodal" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="amodal.pdf" id="Amodal">
              <heading>Amodal Completion and Size Constancy in Natural Scenes</heading></a><br>
              <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2015
              <br></p>

              <div class="paper" id="amodal">
              <a href="javascript:toggleblock('amodal_abs')">abstract</a> /
              <a href="amodal_supp.pdf">supplementary</a> /
              <a shape="rect" href="javascript:togglebib('amodal')" class="togglebib">bibtex</a>
              <p align="justify"> <i id="amodal_abs">We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes.</i></p>

              <pre xml:space="preserve">
@incollection{amodalKarTCM15,
author = {Abhishek Kar and
Shubham Tulsiani and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Amodal Completion and
Size Constancy in Natural Scenes},
booktitle = ICCV,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

		    <tr>
            <td width="33%" valign="top"><a href="basisshapes_highres.png"><img src="basisshapes.png" alt="basisshapes" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="categoryshapes.pdf" id="Category Shapes">
              <heading>Category-Specific Object Reconstruction from a Single Image</heading></a><br>
              <strong>Abhishek Kar*</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani*</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2015 <strong style="color:red">(Oral)</strong><br>
              <strong style="color:red">Best Student Paper Award</strong>
              <br></p>

              <div class="paper" id="categoryshapes">
              <a href="http://cs.berkeley.edu/~akar/categoryShapes">project page</a> /
              <a href="javascript:toggleblock('cat_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('categoryshapes')" class="togglebib">bibtex</a> /
              <a href="categoryshapes_supp.pdf">supplementary</a> /
              <a href="https://github.com/akar43/CategoryShapes">code</a> /
              <a href="http://arxiv.org/abs/1411.6069">arxiv</a>

              <p align="justify"> <i id="cat_abs">Object reconstruction from a single image - in the wild - is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.</i></p>

               <pre xml:space="preserve">
@incollection{categoryShapesKar15,
author = {Abhishek Kar and
Shubham Tulsiani and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Category-Specific Object
Reconstruction from a Single Image},
booktitle = CVPR,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="vvn_highres.png"><img src="vvn.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="vvn.pdf" id="VVN">
              <heading>Virtual View Networks for Object Reconstruction</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2015 <br></p>
              <div class="paper" id="vvn">
              <a href="javascript:toggleblock('vvn_abs')">abstract</a> / <a shape="rect" href="javascript:togglebib('vvn')" class="togglebib">bibtex</a> / <a href="http://youtu.be/JfDJji5sYXE">videos</a> / <a href="http://arxiv.org/abs/1411.6091">arxiv</a>
              <p align="justify"> <i id="vvn_abs">All that structure from motion algorithms “see” are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by com- puting geodesics on novel networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of appli- cations of rigid structure-from-motion techniques may be significantly extended.</i></p>

               <pre xml:space="preserve">
@incollection{vvnCarreira14,
author = {Jo{\~{a}}o Carreira and
Abhishek Kar and
Shubham Tulsiani and
Jitendra Malik},
title = {Virtual View Networks
for Object Reconstruction},
booktitle = CVPR,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="lookingatyou_highres.png"><img src="lookingatyou.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="LookingAtYou.pdf" id="lookingatyou"><heading>Looking At You: Fused Gyro and Face Tracking for Viewing Large Imagery on Mobile Devices</heading></a><br>
              <a href="http://research.microsoft.com/en-us/um/people/neel/">Neel Joshi</a>, <strong>Abhishek Kar</strong>, <a href="http://research.microsoft.com/en-us/um/people/cohen/">Michael F. Cohen</a><br>
              <em>ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)</em>, 2012 <br></p>
              <div class="paper" id="lookatyou">
              <a href="javascript:toggleblock('lay_abs')">abstract</a> / <a shape="rect" href="javascript:togglebib('lookatyou')" class="togglebib">bibtex</a> / <a href="http://research.microsoft.com/en-us/um/redmond/projects/lookingatyou/">website</a> / <a href="chivideo.mp4">video</a>
               <p align="justify"> <i id="lay_abs">We present a touch-free interface for viewing large imagery on mobile devices. In particular, we focus on viewing paradigms for 360 degree panoramas, parallax image sequences, and long multi-perspective panoramas. We describe a sensor fusion methodology that combines face tracking using a front-facing camera with gyroscope data to produce a robust signal that defines the viewer's 3D position relative to the display. The gyroscopic data provides both low-latency feedback and allows extrapolation of the face position beyond the the field-of-view of the front-facing camera. We also demonstrate a hybrid position and rate control that uses the viewer's 3D position to drive exploration of very large image spaces. We report on the efficacy of the hybrid control vs. position only control through a user study.</i></p>

               <pre xml:space="preserve">
@inproceedings{joshi2012looking,
title={Looking at you: fused gyro and face
tracking for viewing large imagery on mobile devices},
author={Joshi, Neel and Kar, Abhishek and Cohen, Michael},
booktitle={Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems},
pages={2211--2220},
year={2012},
organization={ACM}
}
                </pre>
              </div>
            </td>
        </tr>

        </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Tech Showcase</h2>
		</td></tr>
		</table>
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">Local Light Field Fusion</h3>
        <td width="48%" align="center"><iframe width="80%" height=315 src="https://www.youtube.com/embed/Wkfhxj48LIU" frameborder="0" allowfullscreen></iframe>
      </tr>
     </table>   
    <table width="100%" align="center" border="0" cellpadding="20">
	<table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">AR/VR Content Creation</h3>
        <td width="48%" align="center"><iframe width="80%" height=315 src="https://www.youtube.com/embed/j8GAQYh32iU" frameborder="0" allowfullscreen></iframe>
      </tr>
     </table>   
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">Real-time Mobile Style Transfer - <a href="https://itunes.apple.com/us/app/whisky16-live-hd-art-filters-for-photos-video/id1163050983?mt=8">Whisky16</a></h3>
        <td width="48%" align="center"><iframe width="80%" height="315" src="https://www.youtube.com/embed/SRNaq6h0X4g" frameborder="0" allowfullscreen></iframe>
      </tr>
    </table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Other Projects</h2>
		</td></tr>
		</table>
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="33%"><a href="chemstudio_highres.png"><img src="chemstudio.png" width="100%" style="border-style: none"></a>
        <td width="67%" valign="top">
          <p><a href="CS498/Report.pdf" id="chemstudio"><heading>Chemistry Studio: An Intelligent Tutoring System for the Periodic Table</heading></a><br>
          <strong>Abhishek Kar*</strong>, Ankit Kumar*, <a href="http://research.microsoft.com/en-us/um/people/sumitg/">Sumit Gulwani</a>, Ashish Tiwari, <a href="http://www.cse.iitk.ac.in/users/karkare/">Amey Karkare</a><br>
          <em>Undergraduate Thesis, IIT Kanpur</em>, 2012 <br><br>
          <a href="CS498/overview.pdf">slides</a> / <a href="BTP_ppt_1.pptx">talk 1</a> / <a href="BTP_ppt_2.pptx">talk 2</a> </p>
        </td>
    </tr>
    </table>



		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Teaching</h2>
		</td></tr>
		</table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="33%"><img src="cs188.png" alt="pacman" width="100%"></td>

            <td width="67%" valign="center">
              <p>
                <a href="http://www-inst.eecs.berkeley.edu/~cs189"><heading>CS189: Introduction to Machine Learning - Spring 2013 (GSI) </heading></a><br>
                <strong>Instructor</strong>: Prof. Jitendra Malik<br>
                Awarded the <a href="http://gsi.berkeley.edu/programs-services/award-programs/ogsi/">Outstanding GSI Award</a>
              </p>
              <p>
                <a href="http://www-inst.eecs.berkeley.edu/~cs188"><heading>CS188: Introduction to Artificial Intelligence - Spring 2014 (GSI)</heading></a><br>
                <strong>Instructor</strong>: Prof. Pieter Abbeel<br>
              </p>
            </td>
          </tr>

        </table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<br>
        <p align="right"><font size="2">
<a href="http://www.cs.berkeley.edu/~barron/">this guy's website is awesome</a>
</font></p>

</td></tr>
</table>

      </td>
    </tr>
  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vvn_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lay_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('amodal_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('shapesym_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('pamishapes_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('3rcv_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lsm_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sf_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
  hideblock('llff_abs');
  </script>
</body>
</html>
