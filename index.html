
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strongred {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 14px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }
  .center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}


  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Abhishek Kar</title>
  <meta name="Abhishek Kar's Homepage"http-equiv="Content-Type" content="Abhishek Kar's Homepage">

  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src='https://www.google.com/recaptcha/api.js'></script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]//function(){
    (i[r].q=i[r].q//[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-41102261-2', 'auto');
    ga('send', 'pageview');
</script>
</head>

<body>
  <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <p align="center"><font size="7">Abhishek Kar</font><br>
          <tr>
            <td width="67%" valign="middle" align="justify">



		    <p>
I am currently a research and development lead at <a href="https://arvr.google.com/">Google AR.</a> where we work on problems at the intersection of 3D computer vision, computer graphics, computational photography and machine learning. Some features I have worked on and shipped at Google include the <a href="https://developers.google.com/ar/develop/depth">ARCore Depth API</a>, <a href="https://blog.google/products/photos/poem-and-list-so-no-google-photos-memories-are-missed/">Cinematic Memories for Google Photos and Pixel</a> and portrait mode for Google Pixel.<br><br>
Prior to Google, I was the Director of Machine Learning at <a href="https://fyusion.com">Fyusion Inc.</a>, a spatial photography startup based in San Francisco where we shipped multiple 3D technologies including <a href="https://bmild.github.io/llff/">casual light field capture</a>, <a href="https://fyusion.com/inspect3d/">AI-driven damage estimation</a>, creation of <a href="https://venturebeat.com/2016/09/06/fyusion-unveils-mobile-content-creation-platform-for-augmented-reality/">user generated AR/VR content</a> and <a href="https://www.producthunt.com/products/whisky16">real-time style transfer on mobile devices</a>.
I graduated from UC Berkeley in 2017 from <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik's</a> group working on learnt 3D object reconstruction.
I have also spent time at <a href="http://research.microsoft.com/en-us/">Microsoft Research</a> and <a href="https://research.adobe.com/">Adobe Research</a>.

<br>

<p align="center">
<a href="mailto:akar@berkeley.edu">Email</a> / <a href="#">CV</a> / <a href="http://scholar.google.com/citations?user=TIpmrtoAAAAJ&hl=en">Google Scholar</a> / <a href="http://www.linkedin.com/in/abhishekkar/"> LinkedIn </a>
</p>
            </td>
            <td width="33%"valign="top"><img src="images/AK_headshot_circular.png" width="100%"></td>
          </tr>
        </table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr><td>
            <h2>Publications</h2>
            <p align="justify">My primary research interests lie in 3D computer vision and computational photography. Specifically, I am excited about applied research problems with the potential to scale to billions of users.</p>
		</td></tr>
    </table>
        
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      
      <tr>
            <td width="33%" valign="top"><a href="nerfiller.mp4"><img src="nerfiller.gif" width="75%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ethanweber.me/nerfiller/" id="">
                <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none">
              <heading>NeRFiller: Completing Scenes via Generative 3D Inpainting</heading></a><br>
              <a href="https://ethanweber.me/">Ethan Weber</a>,
              <a href="https://holynski.org/">Aleksander Hołyński</a>,
              <a href="https://varunjampani.github.io/">Varun Jampani</a>,
              <a href="https://scholar.google.com/citations?user=WTz38osAAAAJ&hl=en">Saurabh Saxena</a>,
              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
              <strong>Abhishek Kar</strong>, 
              <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
              <br></p>

              <div class="paper" id="nerfiller">
                <a href="https://ethanweber.me/nerfiller/">project</a> /
                <a href="http://arxiv.org/abs/2312.04560">paper</a> /
                <a href="javascript:toggleblock('nerfiller_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('nerfiller')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="nerfiller_abs">We propose NeRFiller, an approach that completes missing portions of a 3D capture via generative 3D inpainting using off-the-shelf 2D visual generative models. Often parts of a captured 3D scene or object are missing due to mesh reconstruction failures or a lack of observations (e.g., contact regions, such as the bottom of objects, or hard-to-reach areas). We approach this challenging 3D inpainting problem by leveraging a 2D inpainting diffusion model. We identify a surprising behavior of these models, where they generate more 3D consistent inpaints when images form a 2x2 grid, and show how to generalize this behavior to more than four images. We then present an iterative framework to distill these inpainted regions into a single consistent 3D scene. In contrast to related works, we focus on completing scenes rather than deleting foreground objects, and our approach does not require tight 2D object masks or text. We compare our approach to relevant baselines adapted to our setting on a variety of scenes, where NeRFiller creates the most 3D consistent and plausible scene completions.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{weber2023nerfiller,
  title = {NeRFiller: Completing Scenes 
    via Generative 3D Inpainting},
  author = {Ethan Weber and 
    Aleksander Holynski and 
    Varun Jampani and 
    Saurabh Saxena and
    Noah Snavely and 
    Abhishek Kar and 
    Angjoo Kanazawa},
  booktitle = {CVPR},
  year = {2024},
}
}
                  </pre>
                </div>
            </td>

        </tr>
      
      <tr>
            <td width="33%" valign="top"><a href="shinobi.gif"><img src="shinobi.gif" width="100%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://shinobi.aengelhardt.com/" id="">
                <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none">
              <heading>SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild</heading></a><br>
              <a href="https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/andreas-engelhardt/">Andreas Engelhardt</a>,
              <a href="https://amitraj93.github.io/">Amit Raj</a>,
              <a href="http://markboss.me">Mark Boss</a>,
              <a href="https://cs.stanford.edu/~yzzhang/">Yunzhi Zhang</a>,
              <strong>Abhishek Kar</strong>, 
              <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
              <a href="http://ricardomartinbrualla.com/">Ricardo Martin Brualla</a>, 
              <a href="https://deqings.github.io/">Deqing Sun</a>, 
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>, 
              <a href="https://scholar.google.com/citations?user=2R22h84AAAAJ&hl=en">Hendrik P. A. Lensch</a>, 
              <a href="https://varunjampani.github.io/">Varun Jampani</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
              <br></p>

              <div class="paper" id="shinobi">
                <a href="https://shinobi.aengelhardt.com/">project</a> /
                <a href="https://www.youtube.com/watch?v=m_5kvtlDnl4">video</a> /
                <a href="https://arxiv.org/abs/2401.10171">paper</a> /
                <a href="javascript:toggleblock('shinobi_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('shinobi')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="shinobi_abs">We present SHINOBI, an end-to-end framework for the reconstruction of shape, material, and illumination from object images captured with varying lighting, pose, and background. Inverse rendering of an object based on unconstrained image collections is a long-standing challenge in computer vision and graphics and requires a joint optimization over shape, radiance, and pose. We show that an implicit shape representation based on a multi-resolution hash encoding enables faster and robust shape reconstruction with joint camera alignment optimization that outperforms prior work. Further, to enable the editing of illumination and object reflectance (i.e. material) we jointly optimize BRDF and illumination together with the object's shape. Our method is class-agnostic and works on in-the-wild image collections of objects to produce relightable 3D assets for several use cases such as AR/VR, movies, games, etc.</i></p>
                <pre xml:space="preserve">
@inproceedings{engelhardt2024-shinobi,
  author = {Engelhardt, Andreas and 
    Raj, Amit and
    Boss, Mark and
    Zhang, Yunzhi and
    Kar, Abhishek and 
    Li, Yuanzhen and 
    Sun, Deqing and 
    Martin Brualla, Ricardo and
    Barron, Jonathan T. and 
    Lensch, Hendrik P.A. and 
    Jampani, Varun},
    title = {SHINOBI: Shape and Illumination
      using Neural Object Decomposition 
      via BRDF Optimization In-the-wild},
    venue={Computer Vision and Pattern Recognition (CVPR)},
  year = {2024}
}
                  </pre>
                </div>
            </td>

        </tr>
      
      <tr>
            <td width="33%" valign="top"><a href="accel_nf.png"><img src="accel_nf.png" width="80%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ubc-vision.github.io/nf-soft-mining/" id="">
                <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none">
              <heading>Accelerating Neural Field Training via Soft Mining</heading></a><br>
              <a href="https://shakibakh.github.io/">Shakiba Kheradmand</a>, 
              <a href="http://drebain.com/">Daniel Rebain</a>,
              <a href="https://hippogriff.github.io/">Gopal Sharma</a>, 
              <a href="http://www.hossamisack.com/">Hossam Isack</a>,
              <strong>Abhishek Kar</strong>, 
              <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
              <br></p>

              <div class="paper" id="accel_nf">
                <a href="https://ubc-vision.github.io/nf-soft-mining/">project</a> /
                <a href="https://arxiv.org/abs/2312.00075">paper</a> /
                <a href="javascript:toggleblock('accel_nf_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('accel_nf')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="accel_nf_abs">We present an approach to accelerate Neural Field training by efficiently selecting sampling locations. While Neural Fields have recently become popular, it is often trained by uniformly sampling the training domain, or through handcrafted heuristics. We show that improved convergence and final training quality can be achieved by a soft mining technique based on importance sampling: rather than either considering or ignoring a pixel completely, we weigh the corresponding loss by a scalar. To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{kheradmand2024softmining,
  title={Accelerating Neural Field Training via Soft Mining},
  year={2024},
  venue={Computer Vision and Pattern Recognition (CVPR)},
  arxiv={https://arxiv.org/abs/2312.00075},
  authors={Shakiba Kheradmand and 
    Daniel Rebain and 
    Gopal Sharma and 
    Hossam Isack and 
    Abhishek Kar and 
    Andrea Tagliasacchi and 
    Kwang Moo Yi}
}
}
      
                  </pre>
                </div>
            </td>

        </tr>
      
      <tr>
            <td width="33%" valign="top"><a href="unsup_kp.gif"><img src="unsup_kp.gif" width="80%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://stablekeypoints.github.io/" id="">
                <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none">
              <heading>Unsupervised Keypoints from Pretrained Diffusion Models</heading></a><br>
              <a href="https://ehedlin.github.io/">Eric Hedlin</a>, 
              <a href="https://hippogriff.github.io/">Gopal Sharma</a>, 
              <a href="https://s-mahajan.github.io/">Shweta Mahajan</a>, 
              <a href="http://www.hossamisack.com/">Hossam Isack</a>,
              <strong>Abhishek Kar</strong>, 
              <a href="https://helge.rhodin.de/index.html/">Helge Rhodin</a>,
              <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024
              <br></p>

              <div class="paper" id="unsup_kp">
                <a href="https://stablekeypoints.github.io/">project</a> /
                <a href="https://arxiv.org/abs/2312.00065">paper</a> /
                <a href="javascript:toggleblock('unsup_kp_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('unsup_kp')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="unsup_kp_abs">Unsupervised learning of keypoints and landmarks has seen significant progress with the help of modern neural network architectures, but performance is yet to match the supervised counterpart, making their practicability questionable. We leverage the emergent knowledge within text-to-image diffusion models, towards more robust unsupervised keypoints. Our core idea is to find text embeddings that would cause the generative model to consistently attend to compact regions in images (i.e. keypoints). To do so, we simply optimize the text embedding such that the cross-attention maps within the denoising network are localized as Gaussians with small standard deviations. We validate our performance on multiple dataset: the CelebA, CUB-200-2011, Tai-Chi-HD, DeepFashion, and Human3.6m datasets. We achieve significantly improved accuracy, sometimes even outperforming supervised ones, particularly for data that is non-aligned and less curated.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{hedlin2024keypoints,
  title={Unsupervised Keypoints from 
    Pretrained Diffusion Models},
  year={2024},
  venue={Computer Vision and Pattern Recognition (CVPR)},
  arxiv={https://arxiv.org/abs/2312.00065},
  authors={Eric Hedlin and 
    Gopal Sharma and 
    Shweta Mahajan and 
    Hossam Isack and 
    Abhishek Kar and 
    Helge Rhodin and 
    Andrea Tagliasacchi and 
    Kwang Moo Yi}
}
}
      
                  </pre>
                </div>
            </td>

        </tr>
      
      <tr>
            <td width="33%" valign="top"><a href="unsup_corr.png"><img src="unsup_corr.png" width="80%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://ubc-vision.github.io/LDM_correspondences/" id="">
              <heading>Unsupervised Semantic Correspondence Using Stable Diffusion</heading></a><br>
              <a href="https://ehedlin.github.io/">Eric Hedlin</a>, 
              <a href="https://hippogriff.github.io/">Gopal Sharma</a>, 
              <a href="https://s-mahajan.github.io/">Shweta Mahajan</a>, 
              <a href="http://www.hossamisack.com/">Hossam Isack</a>,
              <strong>Abhishek Kar</strong>, 
              <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
              <br></p>

              <div class="paper" id="unsup_corr">
                <a href="https://ubc-vision.github.io/LDM_correspondences/">project</a> /
                <a href="https://arxiv.org/abs/2305.15581">paper</a> /
                <a href="javascript:toggleblock('unsup_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('unsup_corr')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="unsup_abs">Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{hedlin2023unsupervised,
  title={Unsupervised Semantic Correspondence
    Using Stable Diffusion},
  author={Eric Hedlin and 
    Gopal Sharma and 
    Shweta Mahajan and 
    Hossam Isack and 
    Abhishek Kar and 
    Andrea Tagliasacchi and 
    Kwang Moo Yi},
  booktitle={arXiv preprint},
  year={2023},
  publisher_page={https://arxiv.org/abs/2305.15581},
  homepage={https://ubc-vision.github.io/LDM_correspondences/}
}
      
                  </pre>
                </div>
            </td>

        </tr>
      
      <tr>
            <td width="33%" valign="top"><a href="https://depth-gen.github.io/assets/meeting.mp4"><img src="depthgen/meeting.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://diffusion-vision.github.io/" id="">
              <heading>The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation</heading></a><br>
              <a href="https://scholar.google.com/citations?user=WTz38osAAAAJ&hl=en">Saurabh Saxena</a>, 
              <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">Charles Hermnann</a>, 
              <a href="https://hurjunhwa.github.io/">Junwa Hur</a>, 
              <strong>Abhishek Kar</strong>, 
              <a href="https://norouzi.github.io/">Mohammad Norouzi</a>,
              <a href="https://deqings.github.io/">Deqing Sun</a>, 
              <a href="https://www.cs.toronto.edu/~fleet/">David J. Fleet</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
              <br></p>

              <div class="paper" id="depthgen">
                <a href="https://diffusion-vision.github.io/">project</a> /
                <a href="https://arxiv.org/abs/2306.01923">paper</a> /
                <a href="https://depth-gen.github.io/">previous version</a> /
                <a href="javascript:toggleblock('depthgen_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('depthgen')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="depthgen_abs">Denoising diffusion probabilistic models have transformed image generation with their impressive fidelity and diversity. We show that they also excel in estimating optical flow and monocular depth, surprisingly, without task-specific architectures and loss functions that are predominant for these tasks. Compared to the point estimates of conventional regression-based methods, diffusion models also enable Monte Carlo inference, e.g., capturing uncertainty and ambiguity in flow and depth. With self-supervised pre-training, the combined use of synthetic and real data for supervised training, and technical innovations (infilling and step-unrolled denoising diffusion training) to handle noisy-incomplete training data, and a simple form of coarse-to-fine refinement, one can train state-of-the-art diffusion models for depth and optical flow estimation. Extensive experiments focus on quantitative performance against benchmarks, ablations, and the model's ability to capture uncertainty and multimodality, and impute missing values. Our model, DDVM (Denoising Diffusion Vision Model), obtains a state-of-the-art relative depth error of 0.074 on the indoor NYU benchmark and an Fl-all outlier rate of 3.26% on the KITTI optical flow benchmark, about 25% better than the best published method.</i></p>
  
                <pre xml:space="preserve">
@misc{saxena2023surprising,
title={The Surprising Effectiveness of Diffusion 
  Models for Optical Flow and Monocular Depth 
  Estimation},
author={Saurabh Saxena and 
  Charles Herrmann and 
  Junhwa Hur and 
  Abhishek Kar and 
  Mohammad Norouzi and 
  Deqing Sun and 
  David J. Fleet},
year={2023},
eprint={2306.01923},
archivePrefix={arXiv},
primaryClass={cs.CV}
}
      
                  </pre>
                </div>
            </td>

        </tr> 

      <tr>
          <td width="33%" valign="top"><a href="https://kampta.github.io/asic/"><img src="asic/asic.gif" width="70%" class="center" style="border-style: none;"></a>
          <td width="67%" valign="top">
            <p><a href="https://kampta.github.io/asic/" id="">
            <heading>ASIC: Aligning Sparse in-the-wild Image Collections</heading></a><br>
            <a href="https://kampta.github.io/">Kamal Gupta</a>, 
            <a href="https://varunjampani.github.io/">Varun Jampani</a>, 
            <a href="https://machc.github.io/">Carlos Esteves</a>, 
            <a href="https://www.cs.umd.edu/~abhinav">Abhinav Shrivastava</a>, 
            <a href="https://www.ameeshmakadia.com/">Ameesh Makadia</a>, 
            <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>, 
            <strong>Abhishek Kar</strong><br>
            <em>International Conference on Computer Vision (ICCV)</em>, 2023
            <br></p>

            <div class="paper" id="asic">
              <a href="https://kampta.github.io/asic/">project</a> /
              <a href="https://arxiv.org/abs/2303.16201">arxiv</a> /
              <a href="javascript:toggleblock('asic_abs')">abstract</a> /
              <a href="https://www.youtube.com/watch?v=fLjkkMriuoY">video</a> /
              <a shape="rect" href="javascript:togglebib('asic')" class="togglebib">bibtex</a>
              <p align="justify"> <i id="asic_abs">We present a method for joint alignment of sparse in-the-wild image collections of an object category. 
                Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection.
                We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compared to existing self-supervised methods.</i></p>
              <pre xml:space="preserve">
@inproceedings{gupta2023asic,
author ={Gupta, Kamal and 
  Jampani, Varun and 
  Esteves, Carlos and 
  Shrivastava, Abhinav and 
  Makadia, Abhinav and 
  Snavely, Noah and 
  Kar, Abhishek},
  title = {ASIC: Aligning Sparse 
    in-the-wild Image Collections},
booktitle={Proceedings of the IEEE 
International Conference on Computer Vision},
  year = {2023},
}
                </pre>
              </div>
          </td>

      </tr>
      <tr>
            <td width="33%" valign="top"><a href="https://people.cs.umass.edu/~zezhoucheng/lu-nerf"><img src="lunerf/lunerf.jpg" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://people.cs.umass.edu/~zezhoucheng/lu-nerf/" id="">
              <heading>LU-NeRF: Scene and Pose Estimation by Synchronizing Local Unposed NeRFs</heading></a><br>
              <a href="http://people.cs.umass.edu/~zezhoucheng">Zezhou Cheng</a>, 
              <a href="https://machc.github.io/">Carlos Esteves</a>, 
              <a href="https://varunjampani.github.io/">Varun Jampani</a>, 
              <strong>Abhishek Kar</strong>, 
              <a href="http://people.cs.umass.edu/~smaji/">Subhransu Maji</a>,
              <a href="https://www.ameeshmakadia.com/">Ameesh Makadia</a><br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2023
              <br></p>

              <div class="paper" id="lunerf">
                <a href="https://people.cs.umass.edu/~zezhoucheng/lu-nerf/">project</a> /
                <a href="http://arxiv.org/abs/2306.05410">paper</a> /
                <a href="javascript:toggleblock('lunerf_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('lunerf')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="lunerf_abs">A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to off-the-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limited assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed mini-scenes. LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based SfM pipelines as it compares favorably to COLMAP on low-texture and low-resolution images.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{cheng2023lunerf,
title={LU-NeRF: Scene and Pose Estimation 
  by Synchronizing Local Unposed NeRFs},
author={Cheng, Zezhou and 
  Esteves, Carlos and 
  Jampani, Varun and 
  Kar, Abhishek and 
  Maji, Subhransu and 
  Makadia, Ameesh},
booktitle={Proceedings of the IEEE 
International Conference on Computer Vision},
year={2023}
}
      
                  </pre>
                </div>
            </td>

        </tr> 
    <tr>
        <td width="33%" valign="top"><a href="https://defocus-control.github.io/videos_compressed/teaser.mp4"><img src="dc2/dc2.gif" width="80%" class="center" style="border-style: none"></a>
        <td width="67%" valign="top">
          <p><a href="https://defocus-control.github.io/" id="">
          <heading>DC^2: Dual-Camera Defocus Control by Learning to Refocus</heading></a><br>
          <a href="https://hadizayer.github.io/">Hadi Alzayer</a>, 
          <a href="https://sites.google.com/view/abdullah-abuolaim/">Abdullah Abuolaim</a>, 
          <a href="https://www.linkedin.com/in/ryan-chan-74599014/">Leung Chun Chan</a>, 
          <a href="https://www.linkedin.com/in/yang-yang-92b5a3160/">Yang Yang</a>, 
          <a href="https://www.linkedin.com/in/ying-chen-lou-1175542/">Ying Chen Lou</a>, 
          <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
          <strong>Abhishek Kar</strong><br>
          <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023
          <br></p>

          <div class="paper" id="dc2">
            <a href="https://defocus-control.github.io/">project</a> /
            <a href="https://t.co/bJnAWGcT8b">arxiv</a> /
            <a href="javascript:toggleblock('dc2_abs')">abstract</a> /
            <a href="https://www.youtube.com/watch?v=q9_tM6_5Nq8">video</a> /
            <a href="https://www.youtube.com/watch?v=3Hs-tyr4FFA">two minute papers</a> /
            <a shape="rect" href="javascript:togglebib('dc2')" class="togglebib">bibtex</a>
            <p align="justify"> <i id="dc2_abs">Smartphone cameras today are increasingly approaching the versatility and quality of professional cameras through a combination of hardware and software advancements. However, fixed aperture remains a key limitation, preventing users from controlling the depth of field (DoF) of captured images. At the same time, many smartphones now have multiple cameras with different fixed apertures -- specifically, an ultra-wide camera with wider field of view and deeper DoF and a higher resolution primary camera with shallower DoF. In this work, we propose DC^2, a system for defocus control for synthetically varying camera aperture, focus distance and arbitrary defocus effects by fusing information from such a dual-camera system. Our key insight is to leverage real-world smartphone camera dataset by using image refocus as a proxy task for learning to control defocus. Quantitative and qualitative evaluations on real-world data demonstrate our system's efficacy where we outperform state-of-the-art on defocus deblurring, bokeh rendering, and image refocus. Finally, we demonstrate creative post-capture defocus control enabled by our method, including tilt-shift and content-based defocus effects.</i></p>
            <pre xml:space="preserve">
@inproceedings{alzayer2023defocuscontrol,
title={DC2: Dual-Camera Defocus Control by 
  Learning to Refocus},
author={Alzayer, Hadi and 
  Abuolaim, Abdullah and 
  Chun Chan, Leung and 
  Yang, Yang and 
  Chen Lou, Ying and 
  Huang, Jia-Bin and 
  Kar, Abhishek},
booktitle={Proceedings of the IEEE/CVF 
  Conference on Computer Vision and 
  Pattern Recognition},
pages={--},
year={2023}
}
              </pre>
            </div>
        </td>

    </tr>

      
      <tr>
            <td width="33%" valign="top"><a href="rapsai/rapsai_teaser.jpg"><img src="rapsai/rapsai_img.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://duruofei.com/projects/rapsai/" id="">
              <heading>Rapsai: Accelerating Machine Learning Prototyping of Multimedia Applications Through Visual Programming</heading></a><br>
              <a href="https://duruofei.com/">Ruofei Du</a>, 
              Na Li, Jing Jin, Michelle Carney, Scott Miles, Maria Kleiner, Xiuxiu Yuan, Yinda Zhang, Anuva Kulkarni, Xingyu "Bruce" Liu, Ahmed Sabie, Sergio Escolano, <strong>Abhishek Kar</strong>, Ping Yu, Ram Iyengar, Adarsh Kowdle, and Alex Olwal
              <em>Proceedings of the Conference on Human Factors in Computing Systems (CHI)</em>, 2023
              <br>
              <strong style="color:red">Best Paper Honorable Mention</strong>
              </p>

              <div class="paper" id="rapsai">
                <a href="https://duruofei.com/projects/rapsai/">project</a> /
                <a href="https://ai.googleblog.com/2023/04/visual-blocks-for-ml-accelerating.html?m=1">blog</a> /
                <a href="https://visualblocksforml.github.io/#/">demo</a> /
                <a href="https://www.youtube.com/watch?v=mQ5mvAbZYvc">video</a> /
                <a href="https://duruofei.com/papers/Du_Rapsai-AcceleratingMachineLearningPrototypingOfMultimediaApplicationsThroughVisualProgramming_CHI2023.pdf">paper</a> /
                <a shape="rect" href="javascript:togglebib('rapsai')" class="togglebib">bibtex</a>
  
                <pre xml:space="preserve">
@inproceedings{Du2023Rapsai,
  title = {{Rapsai: Accelerating Machine Learning Prototyping
    of Multimedia Applications Through Visual Programming}},
  author = {Du, Ruofei and Li, Na and 
    Jin, Jing and Carney, Michelle and 
    Miles, Scott and Kleiner, Maria and 
    Yuan, Xiuxiu and Zhang, Yinda and
    Kulkarni, Anuva and Liu, Xingyu and 
    Sabie, Ahmed and Escolano, Sergio and 
    Kar, Abhishek and Yu, Ping and 
    Iyengar, Ram and Kowdle, Adarsh and 
    Olwal, Alex},
  booktitle = {Proceedings of the 2023 CHI 
    Conference on Human Factors in Computing Systems},
  year = {2023},
  publisher = {ACM},
  series = {CHI},
  doi = {10.1145/3544548.3581338},
}

                  </pre>
                </div>
            </td>

        </tr>    
      
      <tr>
            <td width="33%" valign="top"><a href="samurai/samurai_teaser.webp"><img src="samurai/samurai.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://markboss.me/publication/2022-samurai/" id="">
              <heading>SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections</heading></a><br>
              <a href="http://markboss.me">Mark Boss</a>, 
              Andreas Engelhardt, 
              <strong>Abhishek Kar</strong>, 
              <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a>, 
              <a href="https://deqings.github.io/">Deqing Sun</a>, 
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>, 
              <a href="https://scholar.google.com/citations?user=2R22h84AAAAJ&hl=en">Hendrik P. A. Lensch</a>, 
              <a href="https://varunjampani.github.io/">Varun Jampani</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
              <br></p>

              <div class="paper" id="samurai">
                <a href="https://markboss.me/publication/2022-samurai/">project</a> /
                <a href="https://www.youtube.com/watch?v=LlYuGDjXp-8">video</a> /
                <a href="https://arxiv.org/abs/2205.15768">paper</a> /
                <a href="javascript:toggleblock('samurai_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('samurai')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="samurai_abs">Inverse rendering of an object under entirely unknown capture conditions is a fundamental challenge in computer vision and graphics. Neural approaches such as NeRF have achieved photorealistic results on novel view synthesis, but they require known camera poses. Solving this problem with unknown camera poses is highly challenging as it requires joint optimization over shape, radiance, and pose. This problem is exacerbated when the input images are captured in the wild with varying backgrounds and illuminations. In such image collections in the wild, standard pose estimation techniques fail due to very few estimated correspondences across images. Furthermore, NeRF cannot relight a scene under any illumination, as it operates on radiance (the product of reflectance and illumination). We propose a joint optimization framework to estimate the shape, BRDF, and per-image camera pose and illumination. Our method works on in-the-wild online image collections of an object and produces relightable 3D assets for several use-cases such as AR/VR. To our knowledge, our method is the first to tackle this severely unconstrained task with minimal user interaction.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{boss2022-samurai,
    author = {Boss, Mark and 
      Engelhardt, Andreas and 
      Kar, Abhishek and 
      Li, Yuanzhen and 
      Sun, Deqing and 
      Barron, Jonathan T. and 
      Lensch, Hendrik P.A. and 
      Jampani, Varun},
    title = {{SAMURAI}: {S}hape {A}nd {M}aterial 
    from {U}nconstrained {R}eal-world {A}rbitrary 
    {I}mage collections},
    booktitle = {Advances in Neural Information 
      Processing Systems (NeurIPS)},
    year = {2022}
}
                  </pre>
                </div>
            </td>

        </tr>  
          <tr>
            <td width="33%" valign="top"><a href="vio_init/vio_init.png"><img src="vio_init/vio_init.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://versechow.github.io/monodepth-vio-init" id="">
              <heading>Learned Monocular Depth Priors in Visual-Inertial Initialization</heading></a><br>
              Yunwen Zhou, <strong>Abhishek Kar</strong>, 
              <a href="https://scholar.google.com/citations?user=fQjTKnYAAAAJ&hl=en">Eric Turner</a>, 
              <a href="https://scholar.google.com/citations?user=BtxiLV4AAAAJ&hl=en">Adarsh Kowdle</a>, 
              <a href="https://scholar.google.com/citations?user=PjMyiK8AAAAJ&hl=en">Chao X. Guo</a>, 
              <a href="https://scholar.google.com/citations?user=MIrOs0cAAAAJ&hl=en">Ryan C. DuToit</a>, 
              <a href="https://scholar.google.com/citations?user=5NUgdPcAAAAJ&hl=en">Konstantine Tsotsos</a><br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2022
              <br></p>

              <div class="paper" id="VIOInit">
                <a href="https://versechow.github.io/monodepth-vio-init">project</a> /
                <a href="https://www.youtube.com/watch?v=VYsx7kbJEio">video</a> /
                <a href="https://arxiv.org/abs/2204.09171">paper</a> /
                <a href="javascript:toggleblock('vioinit_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('VIOInit')" class="togglebib">bibtex</a>
		      
                <p align="justify"> <i id="vioinit_abs">Visual-inertial odometry (VIO) is the pose estimation backbone for most AR/VR and autonomous robotic systems today, in both academia and industry. However, these systems are highly sensitive to the initialization of key parameters such as sensor biases, gravity direction, and metric scale. In practical scenarios where high-parallax or variable acceleration assumptions are rarely met (e.g. hovering aerial robot, smartphone AR user not gesticulating with phone), classical visual-inertial initialization formulations often become ill-conditioned and/or fail to meaningfully converge. In this paper we target visual-inertial initialization specifically for these low-excitation scenarios critical to in-the-wild usage. We propose to circumvent the limitations of classical visual-inertial structure-from-motion (SfM) initialization by incorporating a new learning-based measurement as a higher-level input. We leverage learned monocular depth images (mono-depth) to constrain the relative depth of features, and upgrade the mono-depth to metric scale by jointly optimizing for its scale and shift. Our experiments show a significant improvement in problem conditioning compared to a classical formulation for visual-inertial initialization, and demonstrate significant accuracy and robustness improvements relative to the state-of-the-art on public benchmarks, particularly under motion-restricted scenarios. We further extend this improvement to implementation within an existing odometry system to illustrate the impact of our improved initialization method on resulting tracking trajectories.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{verse:monodepth-vio-init-eccv2022,
author = {Yunwen Zhou, 
  Abhishek Kar, 
  Eric Turner, 
  Adarsh Kowdle,
  Chao X. Guo, 
  Ryan C. DuToit, 
  and Konstantine Tsotsos},
title = {Learned Monocular Depth Priors 
  in Visual-Inertial Initialization},
booktitle = {European Conference on Computer Vision},
year = {2022},
}
                  </pre>
                </div>
            </td>

        </tr>  

          <tr>
            <td width="33%" valign="top"><a href="slide/slide_teaser.gif"><img src="slide/slide_teaser.gif" width="90%" class="center" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://varunjampani.github.io/slide/" id="">
              <heading>SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting</heading></a><br>
              <a href="http://varunjampani.github.io"> Varun Jampani* </a>,
 
<a href="https://scholar.google.com/citations?user=eZQNcvcAAAAJ&amp;hl=en"> Huiwen Chang*</a>,
 
<a href="https://www.linkedin.com/in/kyle-sargent-784006134"> Kyle Sargent</a>,
 
<strong>Abhishek Kar</strong>,
 
<a href="https://research.google/people/RichardTucker/"> Richard Tucker</a>,
 
<a href="https://research.google/people/107089/"> Michael Krainin</a>,

<a href="https://www.linkedin.com/in/dominikkaeser"> Dominik Kaeser</a>,
 
<a href="https://billf.mit.edu/"> William T. Freeman</a>,
 
<a href="http://salesin.cs.washington.edu/"> David Salesin</a>,
 
<a href="https://homes.cs.washington.edu/~curless/"> Brian Curless</a>,
 
<a href="https://people.csail.mit.edu/celiu/"> Ce Liu </a>
              <br><em>International Conference on Computer Vision (ICCV)</em>, 2021 <strong style="color:red">(Oral)</strong>
              <br></p>

              <div class="paper" id="SLIDE">
                <a href="https://arxiv.org/abs/2109.01068">paper</a> /
                <a href="https://varunjampani.github.io/slide/">project</a> /
                <a href="https://varunjampani.github.io/papers/jampani21_SLIDE_supp.pdf">supplementary</a> /
                <a href="https://www.youtube.com/watch?v=RQio7q-ueY8">video</a> /
                <a href="javascript:toggleblock('slide_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('SLIDE')" class="togglebib">bibtex</a>
		      

                <p align="justify"> <i id="slide_abs">Single image 3D photography enables viewers to view a still image from novel viewpoints. Recent approaches combine monocular depth networks with inpainting networks to achieve compelling results. A drawback of these techniques is the use of hard depth layering, making them unable to model intricate appearance details such as thin hair-like structures. We present SLIDE, a modular and unified system for single image 3D photography that uses a simple yet effective soft layering strategy to better preserve appearance details in novel views. In addition, we propose a novel depth-aware training strategy for our inpainting module, better suited for the 3D photography task. The resulting SLIDE approach is modular, enabling the use of other components such as segmentation and matting for improved layering. At the same time, SLIDE uses an efficient layered depth formulation that only requires a single forward pass through the component networks to produce high quality 3D photos. Extensive experimental analysis on three view-synthesis datasets, in combination with user studies on in-the-wild image collections, demonstrate superior performance of our technique in comparison to existing strong baselines while being conceptually much simpler.</i></p>
  
                <pre xml:space="preserve">
@inproceedings{jampani:ICCV:2021,
	title = {SLIDE: Single Image 3D Photography with 
  Soft Layering and Depth-aware Inpainting},
	author = {Jampani, Varun and 
  Chang, Huiwen and 
  Sargent, Kyle and 
  Kar, Abhishek and 
  Tucker, Richard and 
  Krainin, Michael and 
  Kaeser, Dominik and 
  Freeman, William T and 
  Salesin, David and 
  Curless, Brian and 
  Liu, Ce},
	booktitle={Proceedings of the IEEE 
  International Conference on Computer Vision},
  year={2021}
}
                  </pre>
                </div>
            </td>

        </tr>
          
          
          <tr>
            <td width="33%" valign="top"><a href="llff.gif"><img src="llff.gif" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://bmild.github.io/llff/" id="">
              <heading>Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines</heading></a><br>
              <a href="http://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall*</a>, <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul Srinivasan*</a>, <a href="https://scholar.google.com/citations?user=yZMAlU4AAAAJ">Rodrigo Ortiz-Cayon</a>, 
              <a href="http://faculty.cs.tamu.edu/nimak/">Nima Khademi Kalantari</a>, <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>, <strong>Abhishek Kar</strong><br>
              <em>SIGGRAPH</em>, 2019
              <br></p>

              <div class="paper" id="LLFF">
                <a href="https://arxiv.org/abs/1905.00889">paper</a> /
                <a href="https://bmild.github.io/llff/">project</a> /
                <a href="https://github.com/Fyusion/LLFF">code</a> /
                <a href="https://youtu.be/LY6MgDUzS3M">video</a> /
                <a href="javascript:toggleblock('llff_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('LLFF')" class="togglebib">bibtex</a> /
		<a href="https://venturebeat.com/2019/07/25/fyusion-unveils-3d-imaging-tech-for-marketers-to-show-off-photorealistic-product-views/">p</a> <a href="https://www.forbes.com/sites/charliefink/2019/07/26/thisweek-in-xr-are-growth-predictions-coming-true">r</a> <a href="https://vfxscience.com/2019/07/27/fyusion-light-field-tech/">e</a> <a href="https://mixed.de/siggraph-2019-fyusion-zeigt-ki-gestuetztes-smartohne-3d-scanning">s</a> <a href="https://www.spar3d.com/uncategorized/fyusion-uses-machine-learning-to-render-realistic-3d-product-views/">s</a>
		      

                <p align="justify"> <i id="llff_abs">We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms.</i></p>
  
                <pre xml:space="preserve">
@article{mildenhall2019llff,
  title={Local Light Field Fusion: Practical View 
  Synthesis with Prescriptive Sampling Guidelines},
  author={Ben Mildenhall and 
  Pratul P. Srinivasan and 
  Rodrigo Ortiz-Cayon and 
  Nima Khademi Kalantari and 
  Ravi Ramamoorthi and 
  Ren Ng and 
  Abhishek Kar},
  journal={ACM Transactions on Graphics (TOG)},
  year={2019}
}

                  </pre>
                </div>
            </td>

        </tr>
          
          <tr>
            <td width="33%" valign="top"><a href="sceneflow/sf-teaser.jpg"><img src="sceneflow/sf-teaser.jpg" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="https://drive.google.com/open?id=1mTBOJefYp6_ZkWwgshcikQ_AI5nw1t7_" id="SF">
              <!-- <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Learning Independent Object Motion from Unlabelled Stereoscopic Videos</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~zhecao/">Zhe Cao</a>, <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~chaene/">Christian H&auml;ne</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2019
              <br></p>

              <div class="paper" id="sf">
		            <a href="https://people.eecs.berkeley.edu/~zhecao/sceneflow/">project</a> /
                <a href="javascript:toggleblock('sf_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('sf')" class="togglebib">bibtex</a> /
                <a href="https://arxiv.org/abs/1901.01971">arxiv</a>

                <p align="justify"> <i id="sf_abs">We present a system for learning motion maps of independently moving objects from stereo videos. The only annotations used in our system are 2D object bounding boxes which introduce the notion of objects in our system. Unlike prior learning based approaches which have focused on predicting dense optical flow fields and/or depth maps for images, we propose to predict instance specific 3D scene flow maps and instance masks from which we derive a factored 3D motion map for each object instance. Our network takes the 3D geometry of the problem into account which allows it to correlate the input images and distinguish moving objects from static ones. We present experiments evaluating the accuracy of our 3D flow vectors, as well as depth maps and projected 2D optical flow where our jointly learned system outperforms earlier approaches trained for each task independently.</i></p>
  
                <pre xml:space="preserve">
@incollection{sfCaoKHM2019,
author = {Zhe Cao and
Abhishek Kar and
Christian H\"ane and
Jitendra Malik},
title = {Learning Independent Object Motion 
from Unlabelled Stereoscopic Videos},
booktitle = CVPR,
year = {2019},
}
  
                  </pre>
                </div>
            </td>

        </tr>
          
          <tr>
            <td width="33%" valign="top"><a href="lsm-teaser.png"><img src="lsm-teaser.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="lsm/lsm_nips17.pdf" id="LSM">
              <!-- <img src="./new.png" alt="[NEW]" width="6%" style="border-style: none"> -->
              <heading>Learning a Multi-View Stereo Machine</heading></a><br>
              <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~chaene/">Christian H&auml;ne</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Neural Information Processing Systems(NIPS)</em>, 2017
              <br></p>

              <div class="paper" id="lsm">
                <a href="javascript:toggleblock('lsm_abs')">abstract</a> /
                <a shape="rect" href="javascript:togglebib('lsm')" class="togglebib">bibtex</a> /
                <a href="lsm/supplsm.pdf">supplementary</a> /
                <a href="https://arxiv.org/abs/1708.05375">arxiv</a> /
                <a href="http://bair.berkeley.edu/blog/2017/09/05/unified-3d/">blog</a> /
                <a href="https://github.com/akar43/lsm">code</a>

  
                <p align="justify"> <i id="lsm_abs">We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.</i></p>
  
                <pre xml:space="preserve">
@incollection{lsmKarHM2017,
  author = {Abhishek Kar and
  Christian H\"ane and
  Jitendra Malik},
  title = {Learning a Multi-View Stereo Machine},
  booktitle = NIPS,
  year = {2017},
  }
  
                  </pre>
                </div>
            </td>

        </tr>

		    <tr>
            <td width="33%" valign="top"><a href="meanshapes.png"><img src="meanshapes.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="pamishapes.pdf" id="PamiShapes">
              <heading>Learning Category-Specific Deformable 3D Models for Object Reconstruction</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani*</a>, <strong>Abhishek Kar*</strong>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2017
              <br></p>

              <div class="paper" id="pamishapes">
              <a href="javascript:toggleblock('pamishapes_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('pamishapes')" class="togglebib">bibtex</a> /
              <a href="categoryShapes/" class="togglebib">project</a>

              <p align="justify"> <i id="pamishapes_abs">We address the problem of fully automatic object localization and reconstruction from a single image. This is both a very challenging and very important problem which has, until recently, received limited attention due to difficulties in segmenting objects and predicting their poses. Here we leverage recent advances in learning convolutional networks for object detection and segmentation and introduce a complementary network for the task of camera viewpoint prediction. These predictors are very powerful, but still not perfect given the stringent requirements of shape reconstruction. Our main contribution is a new class of deformable 3D models that can be robustly fitted to images based on noisy pose and silhouette estimates computed upstream and that can be learned directly from 2D annotations available in object detection datasets. Our models capture top-down information about the main global modes of shape variation within a class providing a ``low-frequency'' shape. In order to capture fine instance-specific shape details, we fuse it with a high-frequency component recovered from shading cues. A comprehensive quantitative analysis and ablation study on  the PASCAL 3D+ dataset validates the approach as we show fully automatic reconstructions on PASCAL VOC as well as large improvements on the task of viewpoint prediction.</i></p>

              <pre xml:space="preserve">
@article{pamishapeTulsianiKCM15,
author = {Shubham Tulsiani and
Abhishek Kar and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Learning Category-Specific Deformable 3D
Models for Object Reconstruction},
journal = {TPAMI},
year = {2016},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="3r.png"><img src="3r.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p>
              <heading>The three R's of computer vision: Recognition, reconstruction and reorganization</heading><br>
              Jitendra Malik, Pablo Arbelaez, Jo&atilde;o Carreira, Katerina Fragkiadaki, <br>Ross Girshick, Georgia Gkioxari, Saurabh Gupta, Bharath Hariharan, <strong>Abhishek Kar</strong>, Shubham Tulsiani<br>
              <em>Pattern Recognition Letters</em>, 2016
              <br></p>

              <div class="paper" id="3rcv">
              <a href="http://www.sciencedirect.com/science/article/pii/S0167865516000313">paper</a> /
              <a href="javascript:toggleblock('3rcv_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('3rcv')" class="togglebib">bibtex</a>

              <p align="justify"> <i id="3rcv_abs">We argue for the importance of the interaction between recognition, reconstruction and re-organization, and propose that as a unifying framework for computer vision. In this view, recognition of objects is reciprocally linked to re-organization, with bottom-up grouping processes generating candidates, which can be classified using top down knowledge, following which the segmentations can be refined again. Recognition of 3D objects could benefit from a reconstruction of 3D structure, and 3D reconstruction can benefit from object category-specific priors. We also show that reconstruction of 3D structure from video data goes hand in hand with the reorganization of the scene. We demonstrate pipelined versions of two systems, one for RGB-D images, and another for RGB images, which produce rich 3D scene interpretations in this framework.</i></p>

              <pre xml:space="preserve">
@article{malik2016three,
title={The three R's of computer vision:
  Recognition, reconstruction and reorganization},
author={Malik, Jitendra and
  Arbel{\'a}ez, Pablo and
  Carreira, Jo{\~a}o and
Fragkiadaki, Katerina and
Girshick, Ross and
Gkioxari, Georgia and
Gupta, Saurabh and
Hariharan, Bharath and
Kar, Abhishek and
Tulsiani, Shubham},
journal={Pattern Recognition Letters},
volume={72},
pages={4--14},
year={2016},
publisher={North-Holland}
}

                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="symmetry.png"><img src="symmetry.png" alt="sym" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="shapesymmetry.pdf" id="ShapeSym">
              <heading>Shape and Symmetry Induction for 3D Objects</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <strong>Abhishek Kar</strong>, <a href="http://ttic.uchicago.edu/~huangqx">Qixing Huang</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>arXiv:1511.07845</em>, 2015
              <br></p>

              <div class="paper" id="shapesym">
              <a href="javascript:toggleblock('shapesym_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('shapesym')" class="togglebib">bibtex</a> /
              <a href="https://arxiv.org/abs/1511.07845">arxiv</a>

              <p align="justify"> <i id="shapesym_abs">Actions as simple as grasping an object or navigating around it require a rich understanding of that object's 3D shape from a given viewpoint. In this paper we repurpose powerful learning machinery, originally developed for object classification, to discover image cues relevant for recovering the 3D shape of potentially unfamiliar objects. We cast the problem as one of local prediction of surface normals and global detection of 3D reflection symmetry planes, which open the door for extrapolating occluded surfaces from visible ones. We demonstrate that our method is able to recover accurate 3D shape information for classes of objects it was not trained on, in both synthetic and real images.</i></p>

              <pre xml:space="preserve">
@incollection{shapeSymTulsianiKHCM15,
author = {Shubham Tulsiani and
Abhishek Kar and
Qixing Huang and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Shape and Symmetry Induction
for 3D Objects},
booktitle = arxiv:1511.07845,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="amodal_highres.png"><img src="amodal.png" alt="amodal" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="amodal.pdf" id="Amodal">
              <heading>Amodal Completion and Size Constancy in Natural Scenes</heading></a><br>
              <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2015
              <br></p>

              <div class="paper" id="amodal">
              <a href="javascript:toggleblock('amodal_abs')">abstract</a> /
              <a href="amodal_supp.pdf">supplementary</a> /
              <a shape="rect" href="javascript:togglebib('amodal')" class="togglebib">bibtex</a>
              <p align="justify"> <i id="amodal_abs">We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes.</i></p>

              <pre xml:space="preserve">
@incollection{amodalKarTCM15,
author = {Abhishek Kar and
Shubham Tulsiani and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Amodal Completion and
Size Constancy in Natural Scenes},
booktitle = ICCV,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

		    <tr>
            <td width="33%" valign="top"><a href="basisshapes_highres.png"><img src="basisshapes.png" alt="basisshapes" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="categoryshapes.pdf" id="Category Shapes">
              <heading>Category-Specific Object Reconstruction from a Single Image</heading></a><br>
              <strong>Abhishek Kar*</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani*</a>, <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2015 <strong style="color:red">(Oral)</strong><br>
              <strong style="color:red">Best Student Paper Award</strong>
              <br></p>

              <div class="paper" id="categoryshapes">
              <a href="http://cs.berkeley.edu/~akar/categoryShapes">project page</a> /
              <a href="javascript:toggleblock('cat_abs')">abstract</a> /
              <a shape="rect" href="javascript:togglebib('categoryshapes')" class="togglebib">bibtex</a> /
              <a href="categoryshapes_supp.pdf">supplementary</a> /
              <a href="https://github.com/akar43/CategoryShapes">code</a> /
              <a href="http://arxiv.org/abs/1411.6069">arxiv</a>

              <p align="justify"> <i id="cat_abs">Object reconstruction from a single image - in the wild - is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC.</i></p>

               <pre xml:space="preserve">
@incollection{categoryShapesKar15,
author = {Abhishek Kar and
Shubham Tulsiani and
Jo{\~{a}}o Carreira and
Jitendra Malik},
title = {Category-Specific Object
Reconstruction from a Single Image},
booktitle = CVPR,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="vvn_highres.png"><img src="vvn.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="vvn.pdf" id="VVN">
              <heading>Virtual View Networks for Object Reconstruction</heading></a><br>
              <a href="http://www.cs.berkeley.edu/~carreira/">Jo&atilde;o Carreira</a>, <strong>Abhishek Kar</strong>, <a href="http://www.cs.berkeley.edu/~shubhtuls/">Shubham Tulsiani</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a><br>
              <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2015 <br></p>
              <div class="paper" id="vvn">
              <a href="javascript:toggleblock('vvn_abs')">abstract</a> / <a shape="rect" href="javascript:togglebib('vvn')" class="togglebib">bibtex</a> / <a href="http://youtu.be/JfDJji5sYXE">videos</a> / <a href="http://arxiv.org/abs/1411.6091">arxiv</a>
              <p align="justify"> <i id="vvn_abs">All that structure from motion algorithms “see” are sets of 2D points. We show that these impoverished views of the world can be faked for the purpose of reconstructing objects in challenging settings, such as from a single image, or from a few ones far apart, by recognizing the object and getting help from a collection of images of other objects from the same class. We synthesize virtual views by com- puting geodesics on novel networks connecting objects with similar viewpoints, and introduce techniques to increase the specificity and robustness of factorization-based object reconstruction in this setting. We report accurate object shape reconstruction from a single image on challenging PASCAL VOC data, which suggests that the current domain of appli- cations of rigid structure-from-motion techniques may be significantly extended.</i></p>

               <pre xml:space="preserve">
@incollection{vvnCarreira14,
author = {Jo{\~{a}}o Carreira and
Abhishek Kar and
Shubham Tulsiani and
Jitendra Malik},
title = {Virtual View Networks
for Object Reconstruction},
booktitle = CVPR,
year = {2015},
}
                </pre>
              </div>
            </td>

        </tr>

        <tr>
            <td width="33%" valign="top"><a href="lookingatyou_highres.png"><img src="lookingatyou.png" width="100%" style="border-style: none"></a>
            <td width="67%" valign="top">
              <p><a href="LookingAtYou.pdf" id="lookingatyou"><heading>Looking At You: Fused Gyro and Face Tracking for Viewing Large Imagery on Mobile Devices</heading></a><br>
              <a href="http://research.microsoft.com/en-us/um/people/neel/">Neel Joshi</a>, <strong>Abhishek Kar</strong>, <a href="http://research.microsoft.com/en-us/um/people/cohen/">Michael F. Cohen</a><br>
              <em>ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)</em>, 2012 <br></p>
              <div class="paper" id="lookatyou">
              <a href="javascript:toggleblock('lay_abs')">abstract</a> / <a shape="rect" href="javascript:togglebib('lookatyou')" class="togglebib">bibtex</a> / <a href="http://research.microsoft.com/en-us/um/redmond/projects/lookingatyou/">website</a> / <a href="chivideo.mp4">video</a>
               <p align="justify"> <i id="lay_abs">We present a touch-free interface for viewing large imagery on mobile devices. In particular, we focus on viewing paradigms for 360 degree panoramas, parallax image sequences, and long multi-perspective panoramas. We describe a sensor fusion methodology that combines face tracking using a front-facing camera with gyroscope data to produce a robust signal that defines the viewer's 3D position relative to the display. The gyroscopic data provides both low-latency feedback and allows extrapolation of the face position beyond the the field-of-view of the front-facing camera. We also demonstrate a hybrid position and rate control that uses the viewer's 3D position to drive exploration of very large image spaces. We report on the efficacy of the hybrid control vs. position only control through a user study.</i></p>

               <pre xml:space="preserve">
@inproceedings{joshi2012looking,
title={Looking at you: fused gyro and face
tracking for viewing large imagery on mobile devices},
author={Joshi, Neel and Kar, Abhishek and Cohen, Michael},
booktitle={Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems},
pages={2211--2220},
year={2012},
organization={ACM}
}
                </pre>
              </div>
            </td>
        </tr>

        </table>
    
    <!--    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Tech Showcase</h2>
		</td></tr>
		</table>
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">Local Light Field Fusion</h3>
        <td width="48%" align="center"><iframe width="80%" height=315 src="https://www.youtube.com/embed/Wkfhxj48LIU" frameborder="0" allowfullscreen></iframe>
      </tr>
     </table>   
    <table width="100%" align="center" border="0" cellpadding="20">
	<table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">AR/VR Content Creation</h3>
        <td width="48%" align="center"><iframe width="80%" height=315 src="https://www.youtube.com/embed/j8GAQYh32iU" frameborder="0" allowfullscreen></iframe>
      </tr>
     </table>   
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <h3 align="center">Real-time Mobile Style Transfer - <a href="https://itunes.apple.com/us/app/whisky16-live-hd-art-filters-for-photos-video/id1163050983?mt=8">Whisky16</a></h3>
        <td width="48%" align="center"><iframe width="80%" height="315" src="https://www.youtube.com/embed/SRNaq6h0X4g" frameborder="0" allowfullscreen></iframe>
      </tr>
    </table>
    -->

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Other Projects</h2>
		</td></tr>
		</table>
    <table width="100%" align="center" border="0" cellpadding="20">
      <tr>
        <td width="33%"><a href="chemstudio_highres.png"><img src="chemstudio.png" width="100%" style="border-style: none"></a>
        <td width="67%" valign="top">
          <p><a href="CS498/Report.pdf" id="chemstudio"><heading>Chemistry Studio: An Intelligent Tutoring System for the Periodic Table</heading></a><br>
          <strong>Abhishek Kar*</strong>, Ankit Kumar*, <a href="http://research.microsoft.com/en-us/um/people/sumitg/">Sumit Gulwani</a>, Ashish Tiwari, <a href="http://www.cse.iitk.ac.in/users/karkare/">Amey Karkare</a><br>
          <em>Undergraduate Thesis, IIT Kanpur</em>, 2012 <br><br>
          <a href="CS498/overview.pdf">slides</a> / <a href="BTP_ppt_1.pptx">talk 1</a> / <a href="BTP_ppt_2.pptx">talk 2</a> </p>
        </td>
    </tr>
    </table>



		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<h2>Teaching</h2>
		</td></tr>
		</table>

        <table width="100%" align="center" border="0" cellpadding="20">
          <tr>
            <td width="33%"><img src="cs188.png" alt="pacman" width="100%"></td>

            <td width="67%" valign="center">
              <p>
                <a href="http://www-inst.eecs.berkeley.edu/~cs189"><heading>CS189: Introduction to Machine Learning - Spring 2013 (GSI) </heading></a><br>
                <strong>Instructor</strong>: Prof. Jitendra Malik<br>
                Awarded the <a href="http://gsi.berkeley.edu/programs-services/award-programs/ogsi/">Outstanding GSI Award</a>
              </p>
              <p>
                <a href="http://www-inst.eecs.berkeley.edu/~cs188"><heading>CS188: Introduction to Artificial Intelligence - Spring 2014 (GSI)</heading></a><br>
                <strong>Instructor</strong>: Prof. Pieter Abbeel<br>
              </p>
            </td>
          </tr>

        </table>

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tr><td>
		<br>
        <p align="right"><font size="2">
<a href="https://jonbarron.info">yet another Jon Barron website</a>
</font></p>

</td></tr>
</table>

      </td>
    </tr>
  </table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('vvn_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('lay_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('amodal_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('shapesym_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('pamishapes_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('3rcv_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lsm_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sf_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
  hideblock('llff_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('slide_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('vioinit_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('samurai_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('depthgen_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('asic_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('dc2_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('lunerf_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('unsup_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('unsup_kp_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('accel_nf_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('shinobi_abs');
  </script>
  <script xml:space="preserve" language="JavaScript">
  hideblock('nerfiller_abs');
  </script>
</body>
</html>